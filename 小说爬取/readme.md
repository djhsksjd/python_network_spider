# 小说网站爬虫程序教学说明（Readme）

## 一、程序说明
本爬虫程序为**纯教学与练习用途**，用于演示从公开小说网站爬取公开章节内容的基础技术逻辑，仅作为爬虫入门学习的参考案例，不涉及商业用途。


## 二、核心原理（教学重点）
1. **请求发送**：通过`requests`库模拟浏览器向小说章节页面发送HTTP GET请求，获取页面HTML源代码（类似“打开网页”的过程）。
2. **数据解析**：用`BeautifulSoup`解析HTML，定位章节标题（如`<h1 class="title">`）和正文（如`<div class="content">`）对应的标签，提取文本内容。
3. **流程控制**：
   - 从小说目录页提取所有章节链接，生成待爬列表；
   - 循环遍历列表，逐个请求章节页面，避免一次性大量请求；
   - 通过`time.sleep(1-2)`设置延时，降低对目标网站的访问压力。
4. **数据存储**：将提取的章节内容按顺序保存到本地TXT文件，方便查看结果。


## 三、爬取步骤（实操演示）


1. **前期准备**：
   - 确认目标小说网站为**公开可访问**，且爬取内容为免费章节；
   - 查看网站`robots.txt`（爬虫协议），仅爬取允许访问的内容。
2. **获取章节链接**：
   - 通过排行榜抓取目录信息.[link](https://book.qq.com/book-cate/0-0-0-0-0-0-0-1)
   ![排行榜目录参数解析](/小说爬取/大目录参数解析.png)
   - 请求小说id，解析HTML中的`div.book-large`标签，提取所有章节的id.
3. **爬取章节内容**：
   - 遍历章节URL列表，依次发送请求，获取每个页面的HTML,如[章节列表https://ubook.reader.qq.com/api/book/chapter-list?bid={element['mulan-bid']}](https://ubook.reader.qq.com/api/book/chapter-list?bid={element['mulan-bid']})；
   - 解析出标题和正文文本，过滤广告、冗余标签（如“点赞关注”等非正文内容）。
4. **保存结果**：
   - 创建本地TXT文件，按“章节标题+正文”的格式写入内容，完成后关闭文件。

### 由于多次测试novel.py，保存在book文件夹中的小说可能有些混乱的地方

## 四、常见问题及解决
| 问题 | 原因 | 解决方法 |
|------|------|----------|
| 爬取内容为空 | 标签定位错误（如类名拼写错误） | 用浏览器F12检查元素，确认标签属性（如`class`值）是否正确 |
| 请求被拒绝（403错误） | 缺乏请求头，被识别为爬虫 | 添加模拟浏览器的请求头：`headers = {"User-Agent": "Mozilla/5.0..."}` |
| 内容乱码 | 页面编码与解析编码不一致 | 在请求后指定编码：`response.encoding = "utf-8"`（根据网站实际编码调整） |
| 爬取被限制 | 请求频率过高触发反爬 | 延长延时（如`time.sleep(2)`），减少单次爬取章节数量 |


## 五、免责声明
1. 本程序仅用于**技术教学和个人练习**，严禁用于爬取付费章节、非公开内容，或对网站进行高频请求、恶意访问。
2. 所有操作需遵守目标网站规则及`robots.txt`协议，不得干扰网站正常运营，不得造成服务器负担。
3. 坚决反对任何利用本程序攻击目标网站、窃取数据、侵犯知识产权的行为，此类行为与本程序及教学目的无关，责任由使用者自行承担。
4. 使用时需严格遵守《网络安全法》《个人信息保护法》等法律法规，技术学习必须以遵守网络道德为前提。

合理使用技术，共同维护网络秩序。